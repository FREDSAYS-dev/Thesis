{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaaoi2UEuDErrF/Lv1qAhA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FREDSAYS-dev/Thesis/blob/main/THESIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kJ6RiHHB-zSD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class SimpleEnv:\n",
        "    \"\"\"\n",
        "    Minimal 1-D environment for NPC experiments.\n",
        "\n",
        "    State: integer in [0, n_positions - 1]. Start at 0; goal at n_positions - 1.\n",
        "    Actions: 0=left, 1=right.\n",
        "    Rewards: base reward (step_cost) each step; goal_bonus at goal.\n",
        "    Penalties: applied in after_step() hook based on flags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_positions=5, step_cost=-0.1, goal_bonus=1.0, seed=42):\n",
        "        self.n_positions = n_positions\n",
        "        self.goal = n_positions - 1\n",
        "        self.step_cost = step_cost\n",
        "        self.goal_bonus = goal_bonus\n",
        "        self.num_actions = 2\n",
        "        self.seed(seed)\n",
        "\n",
        "    def seed(self, seed):\n",
        "        \"\"\"Set the random seed for reproducibility.\"\"\"\n",
        "        import numpy as np\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment to the start state and return it.\"\"\"\n",
        "        self.state = 0\n",
        "        self._history = [self.state]\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Apply the action and return (next_state, reward, done, info).\"\"\"\n",
        "        old_state = self.state\n",
        "        if action == 0:\n",
        "            self.state = max(0, self.state - 1)\n",
        "        elif action == 1:\n",
        "            self.state = min(self.goal, self.state + 1)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action\")\n",
        "\n",
        "        reward = self.goal_bonus if self.state == self.goal else self.step_cost\n",
        "        done = (self.state == self.goal)\n",
        "        info = {'old_state': old_state, 'action': action}\n",
        "        self._history.append(self.state)\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def observe(self, transition):\n",
        "        \"\"\"\n",
        "        Consume a real transition AFTER step has been called externally.\n",
        "\n",
        "        Args:\n",
        "            transition: dict with keys 'state', 'action', 'next_state', 'reward', 'done', 'info'\n",
        "        \"\"\"\n",
        "        # Here you can append transitions to a dataset or compute metrics.\n",
        "        pass\n",
        "\n",
        "    def after_step(self, reward, flags):\n",
        "        \"\"\"\n",
        "        Apply penalties based on flags after calling step().\n",
        "        Args:\n",
        "            reward: the base reward from step().\n",
        "            flags: dict with boolean keys 'collision', 'stall', 'loop'\n",
        "        Returns:\n",
        "            Adjusted reward after penalties.\n",
        "        \"\"\"\n",
        "        penalty = 0.0\n",
        "        if flags.get('collision', False):\n",
        "            penalty -= 1.0\n",
        "        if flags.get('stall', False):\n",
        "            penalty -= 0.2\n",
        "        if flags.get('loop', False):\n",
        "            penalty -= 0.5\n",
        "        return reward + penalty\n",
        "\n",
        "\n",
        "# Rule-based policy: always move right\n",
        "def rule_based_policy(state):\n",
        "    return 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def aspect_log(policy_func):\n",
        "    def wrapper(env, state):\n",
        "        action = policy_func(env, state)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        print(f\"[LOG] state={state}, action={action}, next_state={next_state}, reward={reward}, done={done}\")\n",
        "        return action\n",
        "    return wrapper\n",
        "\n",
        "def aspect_mood(policy_func, mood_start=0.5, epsilon_base=0.3):\n",
        "\n",
        "    import numpy as np\n",
        "    mood = mood_start\n",
        "\n",
        "    def wrapper(env, state, **kw):\n",
        "        nonlocal mood\n",
        "        epsilon = max(0.1, epsilon_base * (1.0 - mood))\n",
        "        # let inner policy use epsilon if it wants\n",
        "        action = policy_func(env, state) if 'epsilon' not in kw else policy_func(env, state, kw['epsilon'])\n",
        "        aux = {\"epsilon\": epsilon, \"mood\": mood}\n",
        "        # mood will be updated next call when caller injects last_reward\n",
        "        if 'last_reward' in kw:\n",
        "            mood = float(np.clip(mood + 0.1 * kw['last_reward'], 0.0, 1.0))\n",
        "            aux[\"mood\"] = mood\n",
        "        return action, aux\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "\n",
        "# Example: wrap policies\n",
        "logged_policy = aspect_log(rule_based_policy)\n",
        "mood_policy = aspect_mood(rule_based_policy)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vwr16-fzz_eG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = SimpleEnv()\n",
        "num_states, num_actions = 5, 2\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "alpha = 0.1  # learning rate\n",
        "gamma = 0.95  # discount factor\n",
        "epsilon = 0.3  # exploration rate\n",
        "\n",
        "transitions = []  # to collect state–action–reward–next_state for a dataset\n",
        "\n",
        "for episode in range(200):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        # ε-greedy action selection\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(num_actions)\n",
        "        else:\n",
        "            action = np.argmax(Q[state])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Q-learning update\n",
        "        best_next_action = np.argmax(Q[next_state])\n",
        "        td_target = reward + gamma * Q[next_state, best_next_action]\n",
        "        Q[state, action] += alpha * (td_target - Q[state, action])\n",
        "        # record transition for dataset\n",
        "        transitions.append({\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'next_state': next_state\n",
        "        })\n",
        "        state = next_state\n",
        "\n",
        "print(\"Learned Q-table:\")\n",
        "print(Q)\n",
        "\n",
        "\n",
        "Q = np.zeros((env.n_positions, env.num_actions))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def q_policy(env, state):\n",
        "    return np.argmax(Q[state])\n",
        "\n",
        "\n",
        "logged_q_policy = aspect_log(q_policy)\n",
        "mood_q_policy = aspect_mood(q_policy)\n",
        "\n",
        "#  Evaluation loop\n",
        "state = env.reset()\n",
        "aux_for_next_call = {}\n",
        "\n",
        "for _ in range(5):\n",
        "    action, aux = mood_q_policy(env, state, **aux_for_next_call)\n",
        "\n",
        "\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "\n",
        "\n",
        "    flags = {\"collision\": False, \"stall\": (next_state == state), \"loop\": False}\n",
        "    reward = env.after_step(reward, flags)\n",
        "    env.observe({\"state\": state, \"action\": action, \"next_state\": next_state,\n",
        "                 \"reward\": reward, \"done\": done, \"info\": info})\n",
        "\n",
        "\n",
        "    aux_for_next_call = {\"last_reward\": reward}\n",
        "\n",
        "    state = next_state\n",
        "    if done:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T0591Li_CyI",
        "outputId": "ca3ece59-d1cd-4521-e692-6771b4741f1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-table:\n",
            "[[0.43406605 0.57212485]\n",
            " [0.40127556 0.70749998]\n",
            " [0.5590303  0.85      ]\n",
            " [0.69124921 1.        ]\n",
            " [0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoAgentEnv:\n",
        "    \"\"\"\n",
        "    Minimal 2-agent 1D world.\n",
        "    - positions: [pos_agent0, pos_agent1], start [0, 2]\n",
        "    - actions per agent: 0=left, 1=right\n",
        "    - goal at position 4\n",
        "    - collision penalty if both land on same non-goal position\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.num_positions = 5\n",
        "        self.goal = 4\n",
        "        self.num_actions = 2\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.positions = [0, 2]\n",
        "        return self.positions.copy()\n",
        "\n",
        "    def step(self, actions):\n",
        "        rewards = [0.0, 0.0]\n",
        "        done = False\n",
        "\n",
        "        # update positions\n",
        "        for i, action in enumerate(actions):\n",
        "            if action == 0:\n",
        "                self.positions[i] = max(0, self.positions[i] - 1)\n",
        "            else:\n",
        "                self.positions[i] = min(self.num_positions - 1, self.positions[i] + 1)\n",
        "\n",
        "        # collision penalty (if not at goal)\n",
        "        if self.positions[0] == self.positions[1] and self.positions[0] != self.goal:\n",
        "            rewards = [-1.0, -1.0]\n",
        "\n",
        "        # goal reward\n",
        "        for i in range(2):\n",
        "            if self.positions[i] == self.goal:\n",
        "                rewards[i] += 1.0\n",
        "                done = True\n",
        "\n",
        "        return self.positions.copy(), rewards, done"
      ],
      "metadata": {
        "id": "Nm6pjUAW72HB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_policy(_env, _pos):\n",
        "    \"\"\"A simple policy for agent 1 (opponent or ally): returns 0 or 1 at random.\"\"\"\n",
        "    return np.random.randint(2)\n",
        "\n",
        "def q_policy_agent0(env, positions, Q):\n",
        "    \"\"\"\n",
        "    Learner’s policy for agent 0 based on a Q‑table.\n",
        "    Only uses the first position (agent 0) to look up the Q‑value.\n",
        "    \"\"\"\n",
        "    pos0 = positions[0]\n",
        "    return int(np.argmax(Q[pos0]))\n",
        "\n",
        "def combined_policy(env, positions, Q, opponent_policy=random_policy):\n",
        "    \"\"\"\n",
        "    Combines the learner’s action for agent 0 and the opponent/helper’s action for agent 1.\n",
        "    Returns a list [action0, action1].\n",
        "    \"\"\"\n",
        "    action0 = q_policy_agent0(env, positions, Q)\n",
        "    action1 = opponent_policy(env, positions[1])\n",
        "    return [action0, action1]\n"
      ],
      "metadata": {
        "id": "VzegQSBnaT75"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_policy(_env, _state_for_that_agent):\n",
        "    return np.random.randint(2)\n",
        "\n",
        "def q_policy_agent0(env, state_pair, Q):\n",
        "    # control only agent0 via its own position; state_pair = [pos0, pos1]\n",
        "    pos0 = state_pair[0]\n",
        "    return int(np.argmax(Q[pos0]))\n",
        "\n",
        "def combined_policy(env, state_pair, Q, opponent_policy=random_policy):\n",
        "    action0 = q_policy_agent0(env, state_pair, Q)\n",
        "    action1 = opponent_policy(env, state_pair[1])\n",
        "    return [action0, action1]"
      ],
      "metadata": {
        "id": "RTpY6qR38Mq8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NActionPolicy:\n",
        "    \"\"\"\n",
        "    Small decision scope: returns an action and metadata about candidates.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_actions=2, greedy=True):\n",
        "        self.num_actions = num_actions\n",
        "        self.greedy = greedy\n",
        "\n",
        "    def __call__(self, env, state, epsilon=0.1):\n",
        "        actions = list(range(self.num_actions))\n",
        "        probs = np.ones(self.num_actions) / self.num_actions\n",
        "        q_values = np.zeros(self.num_actions)  # replace with real Q-values if available\n",
        "\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.choice(actions, p=probs)\n",
        "        else:\n",
        "            action = int(np.argmax(q_values))\n",
        "\n",
        "        aux = {\n",
        "            'candidate_actions': actions,\n",
        "            'probs': probs.tolist(),\n",
        "            'chosen': action\n",
        "        }\n",
        "        return action, aux\n",
        "\n",
        "def aspect_log(policy_func):\n",
        "    \"\"\"\n",
        "    Log wrapper: prints state, chosen action and any extras returned by the policy.\n",
        "    \"\"\"\n",
        "    def wrapper(env, state, **kw):\n",
        "        action, aux = policy_func(env, state, **kw)\n",
        "        print(f\"[LOG] state={state}, action={action}, aux={aux}\")\n",
        "        return action, aux\n",
        "    return wrapper\n",
        "\n",
        "def aspect_mood(policy_func, mood_start=0.5, epsilon_base=0.3):\n",
        "    \"\"\"\n",
        "    Mood wrapper: adjusts exploration rate based on mood.\n",
        "    Mood increases with positive reward and decreases with negative reward.\n",
        "    \"\"\"\n",
        "    mood = mood_start\n",
        "    def wrapper(env, state, **kw):\n",
        "        nonlocal mood\n",
        "        epsilon = max(0.1, epsilon_base * (1.0 - mood))\n",
        "        aux = {'epsilon': epsilon, 'mood': mood}\n",
        "        action, inner_aux = policy_func(env, state, epsilon=epsilon)\n",
        "        aux.update(inner_aux)\n",
        "        # When calling run_episode, update aux['last_reward'] before next call\n",
        "        if 'last_reward' in aux:\n",
        "            mood = float(np.clip(mood + 0.1 * aux['last_reward'], 0.0, 1.0))\n",
        "        return action, aux\n",
        "    return wrapper\n"
      ],
      "metadata": {
        "id": "9SG27EEvBad6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aspect_log_multi(policy_func):\n",
        "    def wrapper(env, state_pair):\n",
        "        actions = policy_func(env, state_pair)\n",
        "        next_state, rewards, done = env.step(actions)\n",
        "        print(f\"[LOG2] state={state_pair}, actions={actions}, next_state={next_state}, rewards={rewards}, done={done}\")\n",
        "        return actions\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "Hy0HZWEd8X8R"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env2 = TwoAgentEnv()\n",
        "Q_agent0 = np.zeros((env2.num_positions, env2.num_actions))  # placeholder; train if needed\n",
        "\n",
        "# wrap combined policy for logging\n",
        "logged_combined = aspect_log_multi(lambda e, s: combined_policy(e, s, Q_agent0))\n",
        "\n",
        "state = env2.reset()\n",
        "for _ in range(10):\n",
        "    _ = logged_combined(env2, state)   # aspect logs + steps\n",
        "    state = env2.positions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "d4YkhUw48vbb",
        "outputId": "00f59020-3051-45a3-ee2b-25788ad8551a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'TwoAgentEnv' object has no attribute 'num_positions'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2714862780.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwoAgentEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mQ_agent0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# placeholder; train if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# wrap combined policy for logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlogged_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maspect_log_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcombined_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_agent0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TwoAgentEnv' object has no attribute 'num_positions'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transitions_df = pd.DataFrame(transitions)\n",
        "print(transitions_df.head())\n",
        "# transitions_df.to_csv('npc_transitions.csv', index=False)  # save for reuse\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noEcXrLE_JQa",
        "outputId": "1e66fc30-f79a-4ded-d219-78503112c8e1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   state  action  reward  next_state\n",
            "0      0       0    -0.1           0\n",
            "1      0       1    -0.1           1\n",
            "2      1       0    -0.1           0\n",
            "3      0       0    -0.1           0\n",
            "4      0       1    -0.1           1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mood = 0.5  # neutral mood\n",
        "state = env.reset()\n",
        "for step in range(15):\n",
        "    # exploration rate depends on mood (higher mood → less exploration)\n",
        "    epsilon_emotion = max(0.1, 1.0 - mood)\n",
        "    if np.random.rand() < epsilon_emotion:\n",
        "        action = np.random.randint(num_actions)\n",
        "    else:\n",
        "        action = np.argmax(Q[state])\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "\n",
        "    # update mood\n",
        "    mood = min(1.0, max(0.0, mood + 0.1 * reward))\n",
        "    print(f\"Step {step+1}: state={state}, action={'right' if action==1 else 'left'}, reward={reward}, mood={mood:.2f}\")\n",
        "    state = next_state\n",
        "    if done:\n",
        "        state = env.reset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc65fn4y_RtX",
        "outputId": "cf038346-48cf-4f88-e33d-b1a1d180dbfa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: state=0, action=right, reward=-0.1, mood=0.49\n",
            "Step 2: state=1, action=right, reward=-0.1, mood=0.48\n",
            "Step 3: state=2, action=right, reward=-0.1, mood=0.47\n",
            "Step 4: state=3, action=left, reward=-0.1, mood=0.46\n",
            "Step 5: state=2, action=right, reward=-0.1, mood=0.45\n",
            "Step 6: state=3, action=right, reward=1.0, mood=0.55\n",
            "Step 7: state=0, action=left, reward=-0.1, mood=0.54\n",
            "Step 8: state=0, action=left, reward=-0.1, mood=0.53\n",
            "Step 9: state=0, action=left, reward=-0.1, mood=0.52\n",
            "Step 10: state=0, action=left, reward=-0.1, mood=0.51\n",
            "Step 11: state=0, action=right, reward=-0.1, mood=0.50\n",
            "Step 12: state=1, action=left, reward=-0.1, mood=0.49\n",
            "Step 13: state=0, action=left, reward=-0.1, mood=0.48\n",
            "Step 14: state=0, action=right, reward=-0.1, mood=0.47\n",
            "Step 15: state=1, action=left, reward=-0.1, mood=0.46\n"
          ]
        }
      ]
    }
  ]
}