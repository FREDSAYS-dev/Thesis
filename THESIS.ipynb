{ "cells": [ { "cell_type": "markdown", "metadata": {}, "source": [ "# Multi-Agent Roles & NPCs Demo Notebook\n", "\n", "This notebook demonstrates a minimal multi-agent environment with flexible agent roles/tags (Ally, Competitive, Dialogue, etc.), allowing for easy experimentation with agents that help or hinder each other via parameterized policies or goals.\n", "\n", "**Features:**\n", "- `NPCPolicy`: Agents/NPCs with roles, names, and parameterizable behavior\n", "- `TwoAgentEnv`: A simple world for two agents with collision and goal logic\n", "- Example policies for Ally, Competitive, Dialogue, etc.\n", "- Easy extension for more complex experiments\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Imports\n", "import numpy as np\n", "import pandas as pd" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Agent Policy Class with Role Tagging" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "class NPCPolicy:\n", "    \"\"\"\n", "    Policy for an agent/NPC with a role (Ally, Competitive, Dialogue, Main, etc.),\n", "    a policy function, and optional goal.\n", "    \"\"\"\n", "    def __init__(self, role=\"Ally\", policy_func=None, goal=None, name=None):\n", "        self.role = role\n", "        self.policy_func = policy_func or (lambda env, state: 0)  # default: always left\n", "        self.goal = goal\n", "        self.name = name or role\n", "\n", "    def act(self, env, state):\n", "        return self.policy_func(env, state)\n", "\n", "    def __repr__(self):\n", "        return f\"NPCPolicy(role={self.role}, name={self.name})\"" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Two-Agent Environment (with Role Metadata)" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "class TwoAgentEnv:\n", "    \"\"\"\n", "    Minimal 2-agent 1D world with role metadata.\n", "    - State: [pos_agent0, pos_agent1] (positions on line of length 5)\n", "    - Actions: 0=left, 1=right\n", "    - Goal at position 4\n", "    - Collision penalty if both agents land on same (non-goal) position\n", "    \"\"\"\n", "    def __init__(self, agent_policies):\n", "        assert len(agent_policies) == 2, \"This environment expects exactly 2 agents.\"\n", "        self.num_positions = 5\n", "        self.goal = 4\n", "        self.agent_policies = agent_policies\n", "        self.reset()\n", "\n", "    def reset(self):\n", "        self.positions = [0, 2]\n", "        return self.positions.copy()\n", "\n", "    def step(self):\n", "        actions = [agent.act(self, pos) for agent, pos in zip(self.agent_policies, self.positions)]\n", "        rewards = [0.0, 0.0]\n", "        done = False\n", "        for i, action in enumerate(actions):\n", "            if action == 0:\n", "                self.positions[i] = max(0, self.positions[i] - 1)\n", "            else:\n", "                self.positions[i] = min(self.num_positions - 1, self.positions[i] + 1)\n", "        if self.positions[0] == self.positions[1] and self.positions[0] != self.goal:\n", "            rewards = [-1.0, -1.0]\n", "        for i in range(2):\n", "            if self.positions[i] == self.goal:\n", "                rewards[i] += 1.0\n", "                done = True\n", "        info = [{\"role\": agent.role, \"name\": agent.name, \"pos\": pos} for agent, pos in zip(self.agent_policies, self.positions)]\n", "        return self.positions.copy(), rewards, done, info" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Example Role Policies" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "def ally_policy(env, state):\n", "    \"\"\"Always move right toward the goal (helpful).\"\"\"\n", "    return 1\n", "\n", "def competitive_policy(env, state):\n", "    \"\"\"Always move left, away from the goal (hindering).\"\"\"\n", "    return 0\n", "\n", "def dialogue_policy(env, state):\n", "    \"\"\"Random action (placeholder for more complex communication).\"\"\"\n", "    return np.random.randint(2)" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Instantiate Agents with Roles" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "agent_main = NPCPolicy(role=\"Main\", policy_func=ally_policy, name=\"main_agent\")\n", "agent_ally = NPCPolicy(role=\"Ally\", policy_func=ally_policy, name=\"ally_npc\")\n", "agent_comp = NPCPolicy(role=\"Competitive\", policy_func=competitive_policy, name=\"competitor_npc\")\n", "agent_dialogue = NPCPolicy(role=\"Dialogue\", policy_func=dialogue_policy, name=\"dialogue_npc\")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Run Experiments: Main Agent with Various NPC Roles" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "def run_experiment(env, max_steps=10):\n", "    state = env.reset()\n", "    log = []\n", "    for step in range(1, max_steps + 1):\n", "        state, rewards, done, info = env.step()\n", "        log.append({'step': step, 'state': state.copy(), 'rewards': rewards.copy(), 'info': info})\n", "        print(f\"Step {step}: State: {state}, Rewards: {rewards}, Info: {info}\")\n", "        if done:\n", "            print(\"\\n--- Episode finished ---\\n\")\n", "            break\n", "    return log" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## NPC Experiment Settings (Setting-01, Setting-02, ..., Setting-N) & Results Tracking\n", "\n", "Define your experiment settings below. Each setting is a tuple: (description, [main agent, npc agent]). Add more settings as needed." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "experiment_settings = [\n", "    (\"Main vs Ally\", [agent_main, agent_ally]),\n", "    (\"Main vs Competitive\", [agent_main, agent_comp]),\n", "    (\"Main vs Dialogue\", [agent_main, agent_dialogue]),\n", "    # Add more settings here (e.g., with parameterized policies or different agent order)\n", "]\n", "\n", "experiment_results = []\n", "\n", "for setting_name, agents in experiment_settings:\n", "    print(f\"\\n=== Running setting: {setting_name} ===\")\n", "    env = TwoAgentEnv(agents)\n", "    log = run_experiment(env, max_steps=20)\n", "    final = log[-1] if log else {}\n", "    experiment_results.append({\n", "        'Setting': setting_name,\n", "        'Final State': final.get('state', None),\n", "        'Final Rewards': final.get('rewards', None),\n", "        'Steps': final.get('step', None),\n", "        'Full Log': log\n", "    })\n", "\n", "# Show summary table\n", "summary = pd.DataFrame([\n", "    {k: v for k, v in result.items() if k != 'Full Log'} for result in experiment_results\n", "])\n", "display(summary)" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Conclusion & Next Steps\n", "\n", "This notebook gives you a foundation for experiments with agent roles, interaction policies, and simple multi-agent environments. You can extend these classes for more agents, richer state/action spaces, or more sophisticated role behaviors as your research demands.\n", "\n", "- Add more rows to `experiment_settings` to automate more experiment variants.\n", "- Use the `Full Log` field in `experiment_results` for deeper analysis (e.g., plotting, statistics).\n", "- Extend policies for learning, communication, or other advanced behaviors." ] } ], "metadata": { "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" }, "language_info": { "name": "python" } }, "nbformat": 4, "nbformat_minor": 2 }