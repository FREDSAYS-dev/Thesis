# --- AI Quality Analysis ---

# Utility: Calculate Manhattan or absolute distance
def _distance(pos1, pos2):
    if isinstance(pos1, (int, float)):
        return abs(pos1 - pos2)
    # Assume 2D or more
    return sum(abs(p1 - p2) for p1, p2 in zip(pos1, pos2))

# 1. "Right decision" = shortest way to goal (greedy)
def is_right_decision(agent_obs, agent_action, goal_pos):
    """
    Returns True if the agent's action moves it closer to the goal position.
    Supports 1D (int) or ND (tuple/list) positions.
    """
    prev_dist = _distance(agent_obs, goal_pos)
    if isinstance(agent_obs, (int, float)):
        new_pos = agent_obs + agent_action
    else:
        new_pos = [o + a for o, a in zip(agent_obs, agent_action)]
    new_dist = _distance(new_pos, goal_pos)
    return new_dist < prev_dist

# 2. Time spent on right decision (steps before first optimal move)
def time_to_first_right_decision(experiment_results, goal):
    times_agent0, times_agent1 = [], []
    for result in experiment_results:
        log = result['Full Log']
        t0 = t1 = None
        for step, entry in enumerate(log):
            obs = entry['obs']
            actions = entry['actions']
            if t0 is None and is_right_decision(obs[0], actions[0], goal):
                t0 = step
            if t1 is None and is_right_decision(obs[1], actions[1], goal):
                t1 = step
            if t0 is not None and t1 is not None:
                break
        times_agent0.append(t0 if t0 is not None else len(log))
        times_agent1.append(t1 if t1 is not None else len(log))
    print(f"Agent 0 avg steps to first right decision: {sum(times_agent0)/len(times_agent0):.2f}")
    print(f"Agent 1 avg steps to first right decision: {sum(times_agent1)/len(times_agent1):.2f}")

# 3. Decision accuracy (frequency of optimal moves)
def decision_accuracy(experiment_results, goal):
    correct0 = correct1 = total0 = total1 = 0
    for result in experiment_results:
        log = result['Full Log']
        for entry in log:
            obs = entry['obs']
            actions = entry['actions']
            if is_right_decision(obs[0], actions[0], goal):
                correct0 += 1
            if is_right_decision(obs[1], actions[1], goal):
                correct1 += 1
            total0 += 1
            total1 += 1
    acc0 = (correct0 / total0) * 100 if total0 else 0
    acc1 = (correct1 / total1) * 100 if total1 else 0
    print(f"Agent 0 Decision Accuracy: {acc0:.1f}%")
    print(f"Agent 1 Decision Accuracy: {acc1:.1f}%")

# 4. Adaptability: recovery after an unexpected event (forced random action)
import random
def adaptability_analysis(experiment_results, goal, event_step=5, possible_actions=None):
    """
    Simulates an unexpected event at `event_step` by forcibly assigning a random action.
    Measures how many steps before the agent resumes right decisions.
    possible_actions: list of valid actions, e.g. [-1,0,1] or [[-1,0],[1,0],[0,-1],[0,1]]
    """
    if possible_actions is None:
        possible_actions = [-1, 0, 1]
    recovery_steps_agent0, recovery_steps_agent1 = [], []
    for result in experiment_results:
        log = result['Full Log']
        # Only if enough steps
        if len(log) > event_step:
            # Simulate random action at event_step (not actually injected in environment, just for analysis)
            t0 = t1 = None
            for step in range(event_step+1, len(log)):
                entry2 = log[step]
                obs2 = entry2['obs']
                actions2 = entry2['actions']
                if t0 is None and is_right_decision(obs2[0], actions2[0], goal):
                    t0 = step - event_step
                if t1 is None and is_right_decision(obs2[1], actions2[1], goal):
                    t1 = step - event_step
                if t0 is not None and t1 is not None:
                    break
            recovery_steps_agent0.append(t0 if t0 is not None else len(log)-event_step)
            recovery_steps_agent1.append(t1 if t1 is not None else len(log)-event_step)
    if recovery_steps_agent0:
        print(f"Agent 0 avg recovery steps after unexpected event: {sum(recovery_steps_agent0)/len(recovery_steps_agent0):.2f}")
    if recovery_steps_agent1:
        print(f"Agent 1 avg recovery steps after unexpected event: {sum(recovery_steps_agent1)/len(recovery_steps_agent1):.2f}")

# 5. Variation in behavior (Shannon entropy over all actions)
from collections import Counter
import math
def action_entropy(actions):
    counts = Counter(tuple(a) if isinstance(a, (list, tuple)) else a for a in actions)
    total = sum(counts.values())
    entropy = -sum((count/total) * math.log2(count/total) for count in counts.values() if count > 0)
    return entropy

def behavior_variation(experiment_results):
    agent0_actions, agent1_actions = [], []
    for result in experiment_results:
        log = result['Full Log']
        agent0_actions += [entry['actions'][0] for entry in log]
        agent1_actions += [entry['actions'][1] for entry in log]
    ent0 = action_entropy(agent0_actions)
    ent1 = action_entropy(agent1_actions)
    print(f"Agent 0 behavior variation (entropy): {ent0:.2f}")
    print(f"Agent 1 behavior variation (entropy): {ent1:.2f}")

# ------- Example usage -------
# Set goal as appropriate for your environment, e.g. goal=4 or goal=[4,4]
# time_to_first_right_decision(experiment_results, goal=4)
# decision_accuracy(experiment_results, goal=4)
# adaptability_analysis(experiment_results, goal=4, event_step=5, possible_actions=[-1,0,1])
# behavior_variation(experiment_results)